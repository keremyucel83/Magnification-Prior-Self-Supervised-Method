#'''Author- Prakash Chandra Chhipa, Email- prakash.chandra.chhipa@ltu.se/prakash.chandra.chhipa@gmail.com, Year- 2022'''

#Every yaml file describes experiments of finetuning in supervised setting. It typically covers all 5 folds and all 4 magnfications in evaluation. 
#Based on configurations, it initilizes pretrained weights from given setting of pretrained model for batch-sisze and epochs on which pretrained was completed.

#Data
"data_path": "/content/drive/MyDrive/DeepLearningProject/breast"
"train_data_portion": "train_60" # possibles values based on data volume "train_20", "train_40", "train_60", "train_80", "train" - means full training data
"val_data_portion": "val_20"
"test_data_portion": "test_20" #evaluation on validation and test set done after traning completion by its own and results are logged in csv file
"magnification_list": ["40X", "100X", "200X", "400X"] #currently actual magnification input logic is not working and it is defined in paython file main fuction manually. IT works with manual input value for all folds.
# CNN to finetune
"encoder":
  "name": "resnet"
  "version": 50
  "fc_dropout": 0.0

#pretrained model to initalize
"pretrained_encoder":
  "method_type": "imagenet"# "imagenet" #other option - "MPCS", "MPSN", so on
  "variant": "OP" #other options - "RP", "FP"
  "initial_weights": "imagenet" #other options - "simclr", "direct", so on - based on whatever pretrained models are available after pretraining
  "batch_size_list": [2] #comma seprated vlaues can be given to prodices mulrutiple results
  "epochs_list": [100, 200, 400, 500, 800, 1000]
  "checkpoint_base_path": "/content/drive/MyDrive/DeepLearningProject/checkpoint/"


#Training
"epochs": 100
"threshold": 100
"batch_size": 1
"early_stopping_patience": 100
"learning_rate":
  "lr_only": 0.0001
  "patience": 5
"weight_decay": 0.0
"optimizer" : "adam" # default and only option implemented is Adam as of now"
"momentum" : 0.9
"augmentation_level": "low" # "augmentation_03" #check augmentation_strategy python file for more alternatives and customization changed by Kerem


#Computationals
"computational_infra":
  "fold_to_gpu_mapping": #incase of smaller GPU and less GPU this settings can be updated
    "fold_0": 0
    "fold_1": 0
    "fold_2": 0
    "fold_3": 0
    "fold_4": 0
  "workers": 8
  
#Logs
"logs":
  "tensorboard_base_path": "/content/drive/MyDrive/DeepLearningProject/logs/fine_tuning_tensorboard_bc/base_path/"
  "tensorboard_file_path": "/content/drive/MyDrive/DeepLearningProject/logs/fine_tuning_tensorboard_bc/file_path/"
  "stats_file_path": "/content/drive/MyDrive/DeepLearningProject/logs/fine_tuning_tensorboard_bc/stat_files/"

#Outcome
"results":
  "result_base_path": "/content/drive/MyDrive/DeepLearningProject/result/fine_tuning/base_path/"
  "result_stats_path": "/content/drive/MyDrive/DeepLearningProject/result/fine_tuning/stats_files/"
  "result_dir_path": "/content/drive/MyDrive/DeepLearningProject/result/fine_tuning/dir_path/"
