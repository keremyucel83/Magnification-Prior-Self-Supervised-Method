{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1scskDmZktcKrc0e5MIaWFF_3DKXRObdg",
      "authorship_tag": "ABX9TyMO0P1BiYNzPkspyjUcsV85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keremyucel83/Magnification-Prior-Self-Supervised-Method/blob/main/Deep_Learning_Final_Project_NotebookVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Packages"
      ],
      "metadata": {
        "id": "ZoE6s07pG6Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --prefix=/opt/intel/ipp ipp-devel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4c1zISko8yi",
        "outputId": "b10d8c71-86cd-4ef4-8030-c3cf0fa71810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipp-devel\n",
            "  Downloading ipp_devel-2021.7.0-py2.py3-none-manylinux1_x86_64.whl (3.7 kB)\n",
            "Collecting ipp-include==2021.7.0\n",
            "  Downloading ipp_include-2021.7.0-py2.py3-none-manylinux1_x86_64.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.9/314.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipp==2021.7.0\n",
            "  Downloading ipp-2021.7.0-py2.py3-none-manylinux1_x86_64.whl (510.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.6/510.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ipp-include, ipp, ipp-devel\n",
            "Successfully installed ipp-2021.7.0 ipp-devel-2021.7.0 ipp-include-2021.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/pytorch/accimage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozrRhVWfpBhj",
        "outputId": "aa54b2f4-1b86-4e21-db10-fdfc1a41eec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pytorch/accimage\n",
            "  Cloning https://github.com/pytorch/accimage to /tmp/pip-req-build-emwy8pmt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/accimage /tmp/pip-req-build-emwy8pmt\n",
            "  Resolved https://github.com/pytorch/accimage to commit 15ec9d4a95060ee54ab80fa3a7ae57a9ca7763ff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: accimage\n",
            "  Building wheel for accimage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accimage: filename=accimage-0.2.0-cp38-cp38-linux_x86_64.whl size=38433 sha256=b6eba7cc8e9c4a4ee6996b03166bc85c80111cfffab083d805e39512e3e4b99a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-klflrts8/wheels/0c/66/93/ec71ef05851627b4b9c6061d15d9ba9973a123fb206dd359ee\n",
            "Successfully built accimage\n",
            "Installing collected packages: accimage\n",
            "Successfully installed accimage-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install empatches\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xjpoj79Xfz2",
        "outputId": "b1dcb6da-f6c0-4e6c-b813-753550e5d17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting empatches\n",
            "  Downloading empatches-0.2.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from empatches) (1.21.6)\n",
            "Building wheels for collected packages: empatches\n",
            "  Building wheel for empatches (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empatches: filename=empatches-0.2.2-py3-none-any.whl size=8415 sha256=ae6c13a759045cf77055ed622b3fedd9e30cafcd3c0e53302b6b29b3b07cc74c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/9d/4a/840a0efff1f1522d1197ebbafc1a72dec27e04e6873717dcd7\n",
            "Successfully built empatches\n",
            "Installing collected packages: empatches\n",
            "Successfully installed empatches-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi62E_X6Yozf",
        "outputId": "04fed294-fd71-4b36-f4df-d49fb2bf2896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spams\n",
            "  Downloading spams-2.6.5.4.tar.gz (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=6.0 in /usr/local/lib/python3.8/dist-packages (from spams) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from spams) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from spams) (1.7.3)\n",
            "Building wheels for collected packages: spams\n",
            "  Building wheel for spams (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spams: filename=spams-2.6.5.4-cp38-cp38-linux_x86_64.whl size=4466435 sha256=4514e121250c81183770e1164c260b5da2ff603f88a6ad08d898c50052692f73\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d6/cd/b79299cdb26aa57760dbbeb096bc8b71b10c69f4bb95dbd16c\n",
            "Successfully built spams\n",
            "Installing collected packages: spams\n",
            "Successfully installed spams-2.6.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install efficientnet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijqYsC9CaeF2",
        "outputId": "77e5672c-0382-4436-832e-61da48dc18f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from efficientnet_pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->efficientnet_pytorch) (4.4.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=1e7601f915d20b8474c66fd402189cff80bbca9ea580afa88fa859b994352d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/b9/90/25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiK8pM9mbnm7",
        "outputId": "e74272ed-6ad9-44b9-dc75-f3114a5dc983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (1.2.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (4.7.0.68)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.7.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations) (6.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations) (4.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2022.10.10)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set System Path as Project Folder to import required libraries from source code"
      ],
      "metadata": {
        "id": "bTHaZWqxHTI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DeepLearningProject/github/src')"
      ],
      "metadata": {
        "id": "D4Yu0jVdH5b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Modules for pretrain"
      ],
      "metadata": {
        "id": "DyZQzqECHZXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Configuration\n",
        "`import bc_config`"
      ],
      "metadata": {
        "id": "PzkfVGSZHfId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import supervised\n",
        "import self_supervised\n",
        "import data"
      ],
      "metadata": {
        "id": "hgNT-M_OH-xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare BreakHis Data Set"
      ],
      "metadata": {
        "id": "Sq5sZrAUuenN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "##This part is commented not to re-fold and re-process images\n",
        "##\n",
        "#prepare_data_breakhis.main()"
      ],
      "metadata": {
        "id": "BnT8OdkZW6P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrain Data for Single CPU"
      ],
      "metadata": {
        "id": "5H6qfOHpIFvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from self_supervised import pretrain_mpcs_single_gpu"
      ],
      "metadata": {
        "id": "R3agbZiNaFwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Train Data Set"
      ],
      "metadata": {
        "id": "HnPMVbiUuadd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python '/content/drive/MyDrive/DeepLearningProject/github/src/self_supervised/pretrain_mpcs_single_gpu.py' \\--config '/content/drive/MyDrive/DeepLearningProject/github/src/self_supervised/experiment_config/single_gpu/mpcs_fp_rn50.yaml'\n",
        "\n"
      ],
      "metadata": {
        "id": "d6ygIeumu2iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install --prefix=/opt/intel/ipp ipp-devel\n",
        "# !pip3 install git+https://github.com/pytorch/accimage"
      ],
      "metadata": {
        "id": "aN7B97FOYRyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U pip ipp-devel"
      ],
      "metadata": {
        "id": "uVeeVdDwW02G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U git+https://github.com/lilohuang/PyTurboJPEG.git"
      ],
      "metadata": {
        "id": "4cWNCKlBZoSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "# MINICONDA_PREFIX=/usr/local\n",
        "# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "# chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "w9egnjKsdBHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %env  LD_LIBRARY_PATH=/opt/intel/ipp/lib:$LD_LIBRARY_PATH"
      ],
      "metadata": {
        "id": "awMIsNpUdS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !conda install -c conda-forge accimage"
      ],
      "metadata": {
        "id": "ZPMSUiwyc1jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from self_supervised import visualization_tsne"
      ],
      "metadata": {
        "id": "-wHboZyZn46K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python  '/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py'  \\--config '/content/drive/MyDrive/DeepLearningProject/github/src/supervised/experiment_config/breakhis_imagenet_rn50.yaml'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeKQ-xQKHP2f",
        "outputId": "7c7cff8b-c6d2-4594-bd56-39dcaa13e9b6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "low\n",
            "datapath: /content/drive/MyDrive/DeepLearningProject/breast\n",
            "fold fold_0\n",
            "train data portion train_60\n",
            "low\n",
            "datapath: /content/drive/MyDrive/DeepLearningProject/breast\n",
            "fold fold_1\n",
            "train data portion train_60\n",
            "low\n",
            "datapath: /content/drive/MyDrive/DeepLearningProject/breast\n",
            "fold fold_2\n",
            "train data portion train_60\n",
            "Process Process-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py\", line 101, in train_model\n",
            "    train_loader  = get_BreakHis_data_loader(\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 313, in get_BreakHis_data_loader\n",
            "    dataset = BreakHis_Dataset(train_path = dataset_path, transform = transform, augmentation_strategy = augmentation_strategy, pre_processing = pre_processing, image_type_list = image_type_list)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 194, in __init__\n",
            "    for patient_dir_name in os.listdir(train_path):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/breast/fold_2/train_60'\n",
            "low\n",
            "datapath: /content/drive/MyDrive/DeepLearningProject/breast\n",
            "fold fold_3\n",
            "train data portion train_60\n",
            "Process Process-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py\", line 101, in train_model\n",
            "    train_loader  = get_BreakHis_data_loader(\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 313, in get_BreakHis_data_loader\n",
            "    dataset = BreakHis_Dataset(train_path = dataset_path, transform = transform, augmentation_strategy = augmentation_strategy, pre_processing = pre_processing, image_type_list = image_type_list)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 194, in __init__\n",
            "    for patient_dir_name in os.listdir(train_path):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/breast/fold_3/train_60'\n",
            "low\n",
            "datapath: /content/drive/MyDrive/DeepLearningProject/breast\n",
            "fold fold_4\n",
            "train data portion train_60\n",
            "Process Process-5:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py\", line 101, in train_model\n",
            "    train_loader  = get_BreakHis_data_loader(\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 313, in get_BreakHis_data_loader\n",
            "    dataset = BreakHis_Dataset(train_path = dataset_path, transform = transform, augmentation_strategy = augmentation_strategy, pre_processing = pre_processing, image_type_list = image_type_list)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 194, in __init__\n",
            "    for patient_dir_name in os.listdir(train_path):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/breast/fold_4/train_60'\n",
            "1179 1232 1186 1063\n",
            "1063\n",
            "Process Process-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py\", line 110, in train_model\n",
            "    val_loader = get_BreakHis_testdata_loader(\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 319, in get_BreakHis_testdata_loader\n",
            "    dataset = BreakHis_Dataset(dataset_path, transform = transform, augmentation_strategy = None, pre_processing=pre_processing, image_type_list = image_type_list)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 194, in __init__\n",
            "    for patient_dir_name in os.listdir(train_path):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/breast/fold_0/test'\n",
            "Process Process-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/finetune_breakhis.py\", line 101, in train_model\n",
            "    train_loader  = get_BreakHis_data_loader(\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 313, in get_BreakHis_data_loader\n",
            "    dataset = BreakHis_Dataset(train_path = dataset_path, transform = transform, augmentation_strategy = augmentation_strategy, pre_processing = pre_processing, image_type_list = image_type_list)\n",
            "  File \"/content/drive/MyDrive/DeepLearningProject/github/src/supervised/apply/datasets.py\", line 203, in __init__\n",
            "    for image_name in os.listdir(path_40x):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/breast/fold_1/train_60/SOB_B_F_14-9133/40X'\n"
          ]
        }
      ]
    }
  ]
}